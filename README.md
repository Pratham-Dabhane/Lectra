# ğŸ§  Lectra - Personalized Learning Bot with RAG

**AI-powered study assistant that learns from your documents**

Upload PDFs, ask questions, get instant answers with source citations. Built with RAG (Retrieval-Augmented Generation) for accurate, context-aware responses.

![Next.js](https://img.shields.io/badge/Next.js-16-black?style=flat&logo=next.js)
![FastAPI](https://img.shields.io/badge/FastAPI-Python-009688?style=flat&logo=fastapi)
![TypeScript](https://img.shields.io/badge/TypeScript-5-blue?style=flat&logo=typescript)
![Supabase](https://img.shields.io/badge/Supabase-Postgres-green?style=flat&logo=supabase)

---

## âœ¨ Features

### ğŸ¯ Core Capabilities
- **ğŸ’¬ Intelligent Chat** - Ask questions about your uploaded documents
- **ğŸ“š Context Memory** - Remembers previous conversations for smarter answers
- **ğŸ“„ Multi-format Support** - PDF, TXT, and more
- **ğŸ” Source Citations** - Every answer shows where it came from
- **âš¡ Fast Responses** - Powered by Groq API (< 2 seconds)
- **ğŸ” Secure** - User authentication and data privacy with Supabase

### ğŸ§  Advanced AI Features
- **Vector Search** - Pinecone-powered semantic document retrieval
- **Chat History** - Auto-saves conversations across sessions
- **Context-Aware** - Uses last 3 chats for better follow-up answers
- **Source Ranking** - Shows relevance scores (65-95% match)

### ğŸ“ Phase 6: Advanced Study Features (NEW!)
- **ğŸ“„ Export to PDF** - Save conversations as formatted PDFs
- **ğŸ”— Cross-Referencing** - Link related concepts across documents
- **ğŸ“Š Study Analytics** - Track progress, streaks, and topic trends
- **ğŸ´ Smart Flashcards** - AI-generated cards with spaced repetition

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Next.js 16 + TypeScript | Modern React framework |
| **Backend** | FastAPI + Python 3.11 | High-performance API |
| **LLM** | Llama 3.3 70B (Groq) | Answer generation |
| **Embeddings** | all-mpnet-base-v2 | Text â†’ 768-dim vectors |
| **Vector DB** | Pinecone | Semantic search |
| **Database** | Supabase Postgres | User data & chat history |
| **Storage** | Supabase Storage | Document files |
| **Auth** | Supabase Auth | User authentication |

**All services have FREE tiers! ğŸ‰**

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Prerequisites
- Node.js 18+ 
- Python 3.11+
- Accounts: [Supabase](https://supabase.com), [Pinecone](https://pinecone.io), [Groq](https://console.groq.com)

### 2ï¸âƒ£ Clone & Install
```bash
git clone https://github.com/Pratham-Dabhane/lectra.git
cd lectra

# Frontend
npm install

# Backend
cd backend
python -m venv .venv
.venv\Scripts\Activate.ps1  # Windows
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure Environment

**Frontend (`.env.local`):**
```env
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key
```

**Backend (`backend/.env`):**
```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_service_role_key
PINECONE_API_KEY=your_pinecone_key
PINECONE_INDEX_NAME=lectra-embeddings
GROQ_API_KEY=your_groq_key
GROQ_MODEL=llama-3.3-70b-versatile
```

### 4ï¸âƒ£ Setup Database

1. Open [Supabase Dashboard](https://supabase.com/dashboard)
2. Go to SQL Editor â†’ New Query
3. Copy & run: `backend/database/migration_phase5.sql`

### 5ï¸âƒ£ Run Application
```bash
# Terminal 1 - Frontend
npm run dev
# â†’ http://localhost:3000

# Terminal 2 - Backend
cd backend
..\.venv\Scripts\python.exe -m uvicorn main:app --reload
# â†’ http://localhost:8000
```

### 6ï¸âƒ£ Test It!
1. Sign up at http://localhost:3000/auth
2. Upload a PDF document
3. Click "Chat" in navbar
4. Ask questions about your document!

ğŸ“– **Full setup guide:** [SETUP.md](SETUP.md)

---

## ğŸ“ Project Structure

```
lectra/
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ auth.tsx           # Login/signup
â”‚   â”œâ”€â”€ index.tsx          # Upload & file list
â”‚   â””â”€â”€ chat.tsx           # Q&A interface
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ FileUpload.tsx     # Drag & drop upload
â”‚   â”œâ”€â”€ FileList.tsx       # Uploaded files display
â”‚   â””â”€â”€ Navbar.tsx         # Navigation
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py            # FastAPI app
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ ingest.py      # Document processing
â”‚   â”‚   â”œâ”€â”€ ask.py         # Q&A endpoint
â”‚   â”‚   â””â”€â”€ history.py     # Chat history
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py       # RAG orchestration
â”‚   â”‚   â”œâ”€â”€ chat_history.py       # Conversation memory
â”‚   â”‚   â”œâ”€â”€ embeddings.py         # Text embeddings
â”‚   â”‚   â””â”€â”€ vector_store.py       # Pinecone operations
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ migration_phase5.sql  # DB schema
â””â”€â”€ documentation/
    â”œâ”€â”€ PHASE_3_SETUP.md
    â”œâ”€â”€ PHASE_4_CHAT_INTEGRATION.md
    â”œâ”€â”€ PHASE_5_MEMORY.md
    â”œâ”€â”€ PHASE_5_IMPLEMENTATION.md
    â””â”€â”€ ARCHITECTURE.md
```

---

## ğŸ¯ How It Works

### Document Upload â†’ Ingestion
```
1. User uploads PDF â†’ Supabase Storage
2. Backend extracts text â†’ Chunks (1000 chars each)
3. Generate embeddings â†’ 768-dim vectors
4. Store in Pinecone with metadata (user_id, file_name, chunk_index)
```

### Question Answering (RAG)
```
1. User asks question â†’ Generate query embedding
2. Search Pinecone â†’ Top 3 most relevant chunks
3. Load last 3 chat messages â†’ Build context
4. Send to Groq LLM â†’ Generate answer
5. Save Q&A to Supabase â†’ Return with sources
```

### Memory & Context
```
- Stores every Q&A in Supabase Postgres
- Retrieves last 3 conversations on new question
- Merges chat history + document chunks
- Enables follow-up questions without re-explaining
```

---

## ğŸ“š Phase Documentation

| Phase | Feature | Documentation |
|-------|---------|--------------|
| **Phase 1-2** | Auth + Upload + Vector DB | `documentation/PHASE_3_SETUP.md` |
| **Phase 3** | Groq LLM Integration | `documentation/ARCHITECTURE.md` |
| **Phase 4** | Chat Interface | `documentation/PHASE_4_CHAT_INTEGRATION.md` |
| **Phase 5** | Memory & History | `documentation/PHASE_5_MEMORY.md` |

---

## ğŸ§ª API Endpoints

### Backend (http://localhost:8000)

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/` | API info |
| GET | `/docs` | Interactive API docs |
| POST | `/api/ingest` | Process uploaded document |
| POST | `/api/ask` | Ask question (returns answer + sources) |
| GET | `/api/history/{user_id}` | Get chat history |
| DELETE | `/api/history/{user_id}` | Clear chat history |

**Example Request:**
```powershell
Invoke-RestMethod -Uri "http://localhost:8000/api/ask" `
  -Method POST `
  -ContentType "application/json" `
  -Body (@{
    query = "What is cybercrime?"
    user_id = "your-user-id"
    top_k = 3
  } | ConvertTo-Json)
```

---

## ğŸ¨ Features in Action

### Smart Follow-up Questions
```
Q1: "What is phishing?"
A1: "Phishing is a cyber attack..."

Q2: "How do I protect myself from it?"
A2: [Knows "it" = phishing without asking]
    "To protect yourself from phishing..."
```

### Persistent Chat History
- Logout â†’ Login â†’ Previous conversations restored
- Click "Clear History" to start fresh
- Conversations auto-saved after every answer

### Source Transparency
Every answer shows:
- ğŸ“„ Source file name
- ğŸ“Š Relevance score (e.g., 79% match)
- ğŸ“ Text excerpt preview

---

## ğŸ”§ Configuration

### Adjust Memory Settings
Edit `backend/.env`:
```env
CHAT_CONTEXT_WINDOW=3    # How many chats to remember (default: 3)
MAX_CHAT_HISTORY=50      # Max chats to store per user (default: 50)
```

### Change LLM Model
```env
GROQ_MODEL=llama-3.3-70b-versatile  # Current
# Alternatives:
# GROQ_MODEL=llama-3.1-70b-versatile
# GROQ_MODEL=llama-3.1-8b-instant (faster)
```

---

## ğŸ› Troubleshooting

### Backend won't start
```bash
# Check Python version
python --version  # Must be 3.11+

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

# Check .env file exists
ls backend/.env
```

### Frontend can't connect
1. Verify backend running on port 8000
2. Check CORS settings in `backend/main.py`
3. Verify `.env.local` has correct Supabase URL

### Chat history not loading
1. Run database migration: `backend/database/migration_phase5.sql`
2. Check Supabase RLS policies are enabled
3. Verify user is authenticated

ğŸ“– **More help:** [documentation/PHASE_5_MEMORY.md#troubleshooting](documentation/PHASE_5_MEMORY.md#troubleshooting)

---

## ğŸš€ Deployment

### Frontend â†’ Vercel
```bash
vercel --prod
# Add environment variables in Vercel dashboard
```

### Backend â†’ Render/Railway
1. Set build command: `pip install -r requirements.txt`
2. Set start command: `uvicorn main:app --host 0.0.0.0 --port $PORT`
3. Add all environment variables

---

## ğŸ—ºï¸ Roadmap

- [x] **Phase 1-2:** User auth + file upload + document ingestion
- [x] **Phase 3:** RAG pipeline with Groq API
- [x] **Phase 4:** Real-time chat interface with sources
- [x] **Phase 5:** Conversation memory & history persistence
- [ ] **Phase 6:** Advanced features
  - [ ] Voice input/output
  - [ ] Export conversations to PDF
  - [ ] Multi-document cross-referencing
  - [ ] Study session analytics
  - [ ] Smart summaries & flashcards

---

## ğŸ‘¤ Author

**Pratham Dabhane**
- GitHub: [@Pratham-Dabhane](https://github.com/Pratham-Dabhane)
- Repository: [Lectra](https://github.com/Pratham-Dabhane/lectra)

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- [Groq](https://groq.com) - Blazing-fast LLM inference
- [Pinecone](https://pinecone.io) - Vector database
- [Supabase](https://supabase.com) - Backend infrastructure
- [Next.js](https://nextjs.org) - React framework

---

**Built with â¤ï¸ for learners everywhere**

â­ Star this repo if you found it helpful!
